% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim.adam.R
\name{adam}
\alias{adam}
\title{Optimize mathematical function using the Adam algorithm}
\usage{
adam(
  f,
  x0,
  max.iter = 100,
  step.size = 0.1,
  phi1 = 0.5,
  phi2 = 0.8,
  stop.grad = .Machine$double.eps
)
}
\arguments{
\item{f}{a (multi-) dimensional function to be eptimized.}

\item{x0}{the starting point of the optimization.}

\item{max.iter}{the maximum number of iterations performed in the optimization.}

\item{step.size}{the step size (sometimes referred to as 'learn-rate') of the optimization.}

\item{phi1}{decay rate for RMS Prop term, i.e. the squared gradients.}

\item{phi2}{decay rate for Momentum term, i.e. the previous gradients.}

\item{stop.grad}{the stop-criterion for the gradient change.}
}
\description{
This functions uses the Adam algorithm to find the minimum of a (multi-)
dimensional mathematical function. The combination considers both, the
average of the previous gradients (Momentum Optimizer) and the average
of the square gradients (RMS Prop), both under exponential decay.
}
